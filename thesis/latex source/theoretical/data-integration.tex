Data integration constitutes the issue of combining data residing at different sources and providing the user with a unified view of these data \cite{Lenzerini:2002:DIT:543613.543644}. 

The goal of data integration is almost always to simplify the access to a range of existing information systems through a central, integrated component with a unified interface for users and applications. Therefore, integrated information systems provide a unified view on the data sources. Existing information systems can be diverse: Classical relational database systems, files, data accessed by web services or HTML formulas, data generating applications or even other integrated information systems \cite[p. 3-4]{DBLP:books/dp/LeserN2006}.\\
Today, data is classified into three diverse categories. Structured data like it is stored in relational databases, have a predefined structure through a schema.
Semi-structured data also have a schema, but they can deviate from the schema. An example of semi-structured data is a XML file without an accompanying XML schema. The third class is unstructured data and contains, as the name implies, no given structure. Typical unstructured data is natural language text \cite[p. 17]{DBLP:books/dp/LeserN2006}.\\
If we speak of diverse information systems we usually mean heterogeneous systems. Heterogeneity exists among data sources as well as between data sources and the integrated system. In most of all integrated information systems only the latter heterogeneity matters, as data sources often do not communicate among themselves. 
To bridge heterogeneity, it is obviously necessary to translate queries and to implement missing functionality in the integrated system. Table \ref{kinds-of-heterogeneity} shows an overview of existing kinds of heterogeneity \cite[p. 60/61]{DBLP:books/dp/LeserN2006}.

\begin{table}[]
\centering
\begin{tabular}{|l|p{0.7\textwidth}|}
\hline
 \textbf{Technical  heterogeneity}  &  includes all problems to realize the access of the data of the data sources technically. This heterogeneity is overcome if the integrated system is able to send a query to a data source and that data source principally understands the query and produces a set of data as a result.\\ \hline
 \textbf{Syntactic  heterogeneity}    &  includes problems in the presentation of data. This heterogeneity is overcome if all data meaning the same are presented equally.\\ \hline
 \textbf{Data model  heterogeneity} &  means problems in the presentation of data of the used data models. This heterogeneity is solved if the data sources and the integrated system use the same data model.\\ \hline
 \textbf{Structural  heterogeneity}    &  includes differences in the structural representation of information. This heterogeneity is solved if semantic identical concepts are also structural equally modeled. \\ \hline
 \textbf{Schematic  heterogeneity} &  Important special case of the structural heterogeneity, whereby there are differences in the used data model.\\ \hline
 \textbf{Semantic  heterogeneity}    &  Includes problems regarding the meaning of used terms and concepts. This heterogeneity is solved if the integrated system and the data source really mean the same by the used names for schema elements. Equal names means consequently equal meaning.\\ \hline
\end{tabular}
\caption{Kinds of heterogeneity (from \cite[p. 60/61]{DBLP:books/dp/LeserN2006}, own translation)}
\label{kinds-of-heterogeneity}
\end{table}

In general, there are two different types of data integration: The materialized integration and the virtual integration. The difference between these two approaches is as follows: At materialized integration the data to be integrated is stored  into the integrated system itself, so on a central point. The data in the data sources remains but for querying the materialized view is used. At virtual integration, the data is only transported from the data source to the integrated system while the query processing. This temporary data is then again discarded. So integration isn't done once but on each query. Of course, an integrated information system can use both principles. Such a system is called hybrid. Both types have in common that a query is processed on a global schema. For the virtual integrated system, the data only exists virtual, thus relations between data sources and the global schema have to be specified and on query time the query has to be split into query schedules. The schedules are responsible to extract the needed information from the different data sources and subsequently merge and transform the data (from \cite[p. 86-88]{DBLP:books/dp/LeserN2006}, own translation).

\section{Heterogeneity}
\textcolor{red}{From: \cite[chapter 3.3 (p.58-78)]{DBLP:books/dp/LeserN2006}}

Information systems providing not the same methods, models and structures for accessing their data are called \textit{heterogeneous}. It is often the case, that distributed systems (and therefore are maintained independently) tend to be heterogeneous. In other words, distribution leads often to heterogeneity but not necessarily. De facto, it is to observe, that data sources tend to be the more heterogeneous the more they are autonomous. Two independent systems will in practice always  be heterogeneous, even if they contain the same kind of data. Heterogeneity arises from different requirements, different developers and different temporal developments. It even occurs if initially identical software systems are used but over time they were adapted to the specific enterprise's needs. This process is called \textit{customizing}. 

Heterogeneity is the main issue of data integration. As a consequence, integrated systems often restrict autonomy of the data sources making them more homogeneous. A common example would be the use of industry-specific standards like common exchange formats, interfaces or communication protocols.

Heterogeneity exists between the data sources as well between the data sources and the integrated system. But often only the latter case is relevant as the data sources often don't communicate among themselves. An example of heterogeneity of data sources and integrated system would be, if the data sources would be provide SQL-access but the integrated would use SPARQL for querying.

It should be clear that in order to overcome heterogeneity, it is necessary to translate queries and the integrated system has to implement functionality, a data source might miss. But this isn't possible in all cases resp. only with big effort. (\textcolor{red}{TODO: Maybe an example?})

As the understanding of heterogeneity is essential for further proceeding, we will go a little more in depth for each kind of heterogeneity:

\subsection{Technical  heterogeneity}
With this kind of heterogeneity are meant differences between information systems, that aren't established by their data or descriptions, but by the possibilities accessing the data. There are different technical layers on which heterogeneity can exist:
\begin{itemize}
\item \textbf{Request function:} query language, parameterized  functions, canned queries
\item \textbf{Query language:} SQL, XQuery, full text search,...
\item \textbf{Exchange format:} binary data, XML, HTML, tabular,...
\item \textbf{communication protocol:} HTTP, JDBC, SOAP,...
\end{itemize}
\subsection{Syntactic  heterogeneity}
Syntactic heterogeneity describes differences in the presentation of the same circumstances. For example different number formats (little endian and big endian), different character encoding (Unicode, ASCII) or different seperators in text files (tab delimited and comma separated values (CSV)). Syntactic heterogeneity therefore can be reduced to technical differences in the presentation of information. Whereas the \textit{synonym problem,} which deals with the representation of equal concepts through different names, is an issue of \textit{semantic heterogeneity}.
\subsection{Data model  heterogeneity}
Structured information systems describes their managed data using schemas in a certain data model. Thus, data model heterogeneity arises, if the integrated system and a data source manage their data in different data models. Important to note is the fact, that data model heterogeneity is independent from semantic differences. Two systems can use two different data models both describing the same circumstance, e.g. describing data in the object oriented or relational data model. Nevertheless it is to observe that differences in the data model often induces semantic heterogeneity.
\textcolor{red}{TODO: Maybe example image of different data model concepts?}
\subsection{Structural  heterogeneity}
Using the same data model doesn't mean that semantic equal data has to be described equally. We speak of structural heterogeneity, if two schemas are different even though they represent the same extract of the real world (i.d. the intension of their schema elements is equal). 

Structural heterogeneity can have many reasons like different preferences of developers, different requirements, using different data models, technical restrictions and so on. This arises from the \textit{design autonomity} of data sources. A Data source has design autonomity if it can freely decide in which way it provides its data, including the data format, the data model, the schema, syntactic represenation of data, the use of key and comprehension systems and the units of values[\cite[p.55]{DBLP:books/dp/LeserN2006}].

\subsection{Schematic  heterogeneity}
A special case of structural heterogeneity is schematic heterogeneity. Schematic heterogeneity is present if different elements of the data model are used to model the same circumstance. In the relational model it is possible to model information as relations, as attributes or as values, for instance. 

Schematic conflicts are particular difficult to resolve, as it isn't usually possible to overcome them with the query language. So is it mandatory to explicitly state attributes and relations in relational languages. If schematic heterogeneous data sources should be integrated, one would define a view. But the query has to be altered if something in these elements is changed.

\subsection{Semantic  heterogeneity}
Sole values have no implicit meaning in an information system. Looking at the number '20' on a table doesn't allow conclusions concerning its meaning. In fact, it could be anything. Only through \textit{interpretation} data becomes information, and for interpretation one needs knowledge about the concrete use case and world knowledge. The interpretation of data is also called its \textit{semantic}. To interpret data in a information system, further information is therefore consulted:
\begin{itemize}
\item The name of the schema element containing the datum
\item The position of the schema element within the schema
\item Other data values stored in the schema element
\end{itemize}
Comprehending all this information, we speak from the data's \textit{context}. Data is obtaining only its meaning by taking into account its context. But it should be considered that some parts of the context are given in machine-readable form (like the schema), and some not (e.g. domain specific knowledge). Also absolute identical schemas having different contexts can have different semantic meanings. Owing to circumstances, an integrated system has to make implicit context knowledge explicit e.g. by introducing new schema elements.

Semantic conflicts relate to the interpretation of names resp. symbols. Semantic conflicts occur in the interaction of names and concepts in different systems. A \textit{concept} means here the intension of a name, the set of real world objects of a name is called its extension. The most common semantic conflicts are synonyms and homonyms. Two names are \textit{synonyms} if they have the same intension, but are syntactic different. Whereas two names are \textit{homonyms} if they are syntactic identical, but their intensions are different. But more difficult to treat are names, whose intensions neither are identical nor are completely different, but conclude themselves or are overlapping.

In principle, it is difficult to detect semantic conflicts and resolve them clearly. Reasons are that for schema analysis are only available the schema themselves and some example data. Sometimes there is additionally some domain knowledge and documentation of the data sources, but that isn't always the case, particular on autonomous systems. Particular cases occurs always if the concepts aren't clearly defined or are interpreted differently. 

On the other hand, complete resolving isn't always possible, only with big effort achievable or  simple not necessary. This is also called \textit{remaining blur}.

\section{Distribution}
\textcolor{red}{From: \cite[chapter 3.1 (p.51-54)]{DBLP:books/dp/LeserN2006}}

An issue of data integration is the distribution of data to be integrated. We speak thereby of data which lies on different systems. It should be noted that it is assumed that the access to the data is ensured, i.d. the systems are connected. A common example of distribution are data sources which can be accessed by a web interface using a browser: The Browser shows only a snippet of the data provided by a web server. The data itself is managed by the web server or one of the headed data sources. 
Data distribution isn't plainly an annoyance as it is often an intended design decision. Data distribution is handy for load balancing, reliability and protection against data loss. Thereby is the distribution controlled on a central point. The consistency of data is assured using sophisticated mechanisms like the 2-phase.commit protocol. Contrary, in a typical data integration project, distribution of data has historical evolved or is conditioned organizational and is thus uncontrolled redundant.

Distribution can be structured into logical and physical distribution. Data is \textit{physical} distributed, if it lies onto physical different systems which can also geographically be located on different places. In turn, data is \textit{logical} distributed, if there exists multiple possible locations for storing a datum. 

\subsection{Physical distribution}
To integrate physical distributed data there have to be overcome several issues: The first step is to detect the physical storage location of the data. Therefore computer, server and port and network have to be identifiable and locatable. For identification, applications used in the internet use Uniform Resource Locators (URLs) to identify remote computers and services not knowing there exact location. The part of locating is the responsibility of the network layer of the computers, using protocols like TCP/IP.

The second problem resulting from physical distribution is data stored in different schemas. Common query languages don't provide the possibility to query tables using different schemas. There are two possible ways to overcome this hurdle:
\begin{itemize}
\item Separating a query into several schema specific queries and consolidating the results in the integrated system.
\item Developing a language able to handle several schemas in a posed query.
\end{itemize}

The third problem are changed requirements for the query optimizer: In a central database the optimizer's key function is minimizing  the accesses to the secondary storage whereas for a distributed database the accesses over the network should be held as small as possible for this kind of accesses needs considerably more time than accesses to the secondary storage.  

\subsection{Logical distribution}
If identical data is located on different places inside the system, we speak of a logical distribution. The key property of logical distribution is therefore the \textit{overlapping of the intension} of different data storage locations. Thereby it is insignificant where these systems physically really are. Two database instances on one computer induce already distribution issues.

A central point here plays the redundancy in a system. A system is called redundant, if semantic equal data can be located at different places. In order to get a consistent view of a redundant system, redundancy has to be strictly controlled (e.g. using triggers or replication mechanisms). At anytime it has to be assured that the same data is at all different places. But data integration typical has to deal with \textit{uncontrolled redundancy} for each data source is maintained independently. This fact results to several problems:
\begin{itemize}
\item \textbf{Localization}: For a user it isn't obvious in which sources to find specific data. As a result, the integrated system has to provide meta data allowing \textit{to locate data}. This can e.g. done by a catalog of all schemas and their descriptions or a global schema with mappings to source schemas.
\item \textbf{Duplicates}: If it is possible that data exists on multiple locations, there will exist duplicates, i.d. objects stored on both locations. These objects have to be recognized by the integrated system.
\item \textbf{Inconsistencies}: Redundant data can contain inconsistencies, which have to be resolved for a homogeneous presentation.
\end{itemize}

\section{Architectures}

Integrated systems evolved from distributed databases which in turn are based on the classical monolithic database. In the following basic database concepts, on which integrated systems base, are explained. Additionally different types of integrated systems are presented shortly.

The \textbf{monolithic database} is subdivided into three layers as seen in \ref{MonolithicDatabaseArchitecture}: The \textit{internal (or physical) view} is responsible for the storage of the data on the respective database. One layer upwards comes the \textit{conceptual (or logical) view} modeling the data on a conceptual way. The conceptual schema defines which data model is used, which data is stored in the DBMS and the relations among the data.
Splitting the data into the internal and conceptual view, makes the data independent from the physical storage medium. The top layer is called \textit{external (or export) view} and is being concerned with modeling of data as well as the conceptual view. However it isn't modeled the whole application domain. The external view only specifies which part of the conceptual schema is provided to the respective application. An export schema is initially a subset of the conceptual schema, but can be transformed and aggregated. In the external view are defined access restrictions, too. Splitting up conceptual and external view ensures \textit{logical data independence} (\cite[p. 84/85]{DBLP:books/dp/LeserN2006}, own translation).
\begin{figure}[H]
	\begin{center}
		\includegraphics[scale=0.5]{figures/monolithicalDatabaseArchitecture.pdf}
	\end{center}
	\caption{Architecture of a monolithic database \textcolor{red}{TODO: own figure but inspired by book; cite properly}}
	\label{MonolithicDatabaseArchitecture}
\end{figure}
Next comes the \textbf{distributed database} architectures as shown in \ref{DistributedDatabaseArchitecture}. The idea is to distribute the data onto several systems (physical and logical), but a user should be able to query all data at once. In order to achieve this, the architecture is subdivided into \textit{four layers}. Each data source owns a local internal and local conceptual schema. The latter only mirrors the data managed by the local database. Dependent on the used distribution strategy, the local conceptual schema is equal to the global conceptual schema  or an extract from it. Common distribution strategies are vertical and horizontal partitioning: 
\begin{itemize}
\item horizontal partitioning: Data of big tables is distributed per tuples on different computers. A union operation consolidates these parts again.
\item vertical partitioning: Data of big tables is distributed per attributes on different computers. Each partition contains additionally a shared key attribute. This makes it possible to consolidate the partitions with a join operation.
\end{itemize}
On top of the local conceptual schemes stands a global conceptual schema. This schema models the whole application domain and is the central point of reference for the external schemes playing the same role as in the three-layers-architecture.
Distributed databases are close coupled. They are strictly checked while conception and operation and thus the main problems of heterogeneity (like structural and semantic heterogeneity) don't occur (\cite[p. 91-93]{DBLP:books/dp/LeserN2006}, own translation).
\begin{figure}[H]
	\begin{center}
		\includegraphics[scale=0.5]{figures/distributedDatabaseArchitecture.pdf}
	\end{center}
	\caption{Architecture of a distributed database}
	\label{DistributedDatabaseArchitecture}
\end{figure}
For allowing to connect heterogeneous databases, \textbf{Multidatabase Systems} (MBS) have been evolved (shown in \ref{MBSDatabaseArchitecture}). MBS are collections of autonomous databases being loosely linked. Each database grants external applications access to its data. The access is done using a database language which allows to query several databases in one query. Such a language is called multidatabase language. To obtain the autonomy  of the involved databases, a MBS has no global conceptual schema. Instead, each local database keeps an export schema defining which part of the local conceptual schema is provided to external applications.  
It is assumed that no data model heterogeneity is contained in a MBS, i.d. all databases use the same data model or either the multidatabase language or the local data source provide a translation to the global data model. Now, each application can create its own external schema, which integrates one or more data sources. So its the task of the application doing the integration task. A MBS provides only a suitable language for querying (\cite[p. 93/94]{DBLP:books/dp/LeserN2006}, own translation).	
\begin{figure}[H]
	\begin{center}
		\includegraphics[scale=0.5]{figures/MultidatabaseArchitecture.pdf}
	\end{center}
	\caption{Architecture of a  multidatabase system}
	\label{MBSDatabaseArchitecture}
\end{figure}
In contrast to MBS, \textbf{federated database management systems} (FDBMS) have a global conceptual schema as seen in \ref{FDBMSArchitecture}. This schema is the central and stable point of reference for all external schemes and there applications. But in contrast to distributed databases the global schema results after the local schemes with the goal to provide an integrated view of existing and heterogeneous data sets. Data sources keep a high degree of autonomy.
The used data model in the global scheme is known as canonical data model. Every local export schema has to be mapped to this global schema. Thus, in order to support heterogeneity in the data model, another layer is needed between local conceptual schema and the local export schema. This new layer is called \textit{local component schema} and translates the local conceptual schema into the canonical data model. The translation is done by a software component called \textit{Wrapper}. All in all, a FDBMS consists of five different layers.
The global schema can be created either by schema integration or schema mapping. Both variants are introduced later \textcolor{red}{(TODO)}
(\cite[p. 94/95]{DBLP:books/dp/LeserN2006}, own translation).\\
\begin{figure}[H]
	\begin{center}
		\includegraphics[scale=0.5]{figures/federatedDatabaseArchitecture.pdf}
	\end{center}
	\caption{Architecture of a  federated database management systems (FDBMS)}
	\label{FDBMSArchitecture}
\end{figure}

\textbf{Mediator-based information systems} (as seen in \ref{MediatorBasedArchitecture}) are a generalization of the previous architectures, as they know only two separate components, namely Wrappers and Mediators: Wrappers are software components responsible for the access to a solely data source. A Wrapper has to break down technical, data model, schematic and interface heterogeneity. It realizes the communication between the mediators and data sources. To its task count:
\begin{itemize}
\item overcoming interface heterogeneity (e.g. SQL to html formulars)
\item overcoming language heterogeneity. This includes the handling of restricted data sources and different query languages.
\item Providing data model transparency through translating the data into the canonical data model.
\item resolving schema heterogeneity by using a suitable mapping between the source schema and the global schema.
\item Supporting the global query optimizing by providing information about the query capabilities of the data source and expected costs.
\end{itemize}

As the system to be created within this thesis uses wrappers, we go more in depth and look at the design, architecture and implementation goals of an architecture using wrappers. The following statements are results of from \cite{Roth:1997:DSW:645923.670992}:


\textbf{Low start-up costs:} Writing a wrapper should be possible to do very quickly, very simple wrappers even in matters of hours. The authoring of a wrapper should also be as simple as possible, so that the wrapper can be written with little or no knowledge about the internal structure of the integrated system.

\textbf{Easy evolving:} Evolving should be done very easy due to two reasons: A wrapper should be implemented very fast to show feasibility. More sophisticated features a data source can provide, should be added later. Additionally can the data source change  itself over time and the wrapper should be easily adaptable. 


On the other hand are mediators software components, which use knowledge of certain data to \textit{create and provide} information for other applications. In a mediator-based architecture, mediators access one or more wrappers and deliver a specific value, normally structural and semantic data integration.

The following list shows an extract of the services, a mediator might provide (from, \cite[p. 5-6]{Wiederhold1996TheCB}): 
\begin{itemize}
\item Selection of likely relevant material
\item Invocation of wrappers to deal with legacy sources
\item Resolution of domain term	terminology and ontology differences
\item Sending the information and meta-data to the customer application
\item Imposition of security filters to guard private data
\item Omission of replicated information
\item Assessment of quality of material from diverse sources
\item Integration of material from diverse source domains based on join keys
\end{itemize}

In this architecture, data sources usually don't know of the existence of the integrated system, and so autonomy is preserved for all data sources (\cite[p. 97]{DBLP:books/dp/LeserN2006}, own translation).
\begin{figure}[H]
	\begin{center}
		\includegraphics[scale=0.5]{figures/MediatorBasedArchitecture.pdf}
	\end{center}
	\caption{Architecture of a  mediator-based information system}
	\label{MediatorBasedArchitecture}
\end{figure}
In \textbf{peer data management systems} (PDMS) there is no separation between data source and integration system. Query can be posed from every system to every other system within the integrated system. The other system than tries to calculate responses with own or other sourced data. So each participant of the integrated system, a so-called peer, is a mediator and a data source at the same time.  

\textbf{Ontology-based integration} is an approach of semantic integration using ontologies. This approach assumes that the discourse range is able to be specified so exact, that semantic heterogeneity can be solved in a formal model by logical inference. Hereby, the formal model is called ontology. Ontologies define the vocabulary describing all concepts of the field of application, and the relations between these concepts. At the same time, an ontology often serves as global schema of the integration layer. For specifying ontologies, special classes of logics are used, the so-called description logics. With description logics, classes of a domain and relations among them can be specified much more precise than with the relational data model. (\cite[p. 267]{DBLP:books/dp/LeserN2006}, own translation).\\